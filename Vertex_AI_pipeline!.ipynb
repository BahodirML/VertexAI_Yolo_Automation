{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BahodirML/Coding_Practices/blob/main/Vertex_AI_pipeline!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --no-cache-dir --upgrade \"kfp>2\" \\\n",
        "                                        google-cloud-aiplatfor"
      ],
      "metadata": {
        "id": "3RibhJFtfzi-",
        "outputId": "77515fce-ab63-43ac-8eba-2c9b395d059e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kfp>2 in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-cloud-aiplatfor (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-cloud-aiplatfor\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kfp"
      ],
      "metadata": {
        "id": "IaHyUhIFQnAE",
        "outputId": "85c6adbb-b073-411b-a188-1ac7b89f44ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp\n",
            "  Downloading kfp-2.7.0.tar.gz (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.8.0)\n",
            "Collecting kfp-pipeline-spec==0.3.0 (from kfp)\n",
            "  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
            "Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp)\n",
            "  Downloading kfp-server-api-2.0.5.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kubernetes<27,>=8.0.0 (from kfp)\n",
            "  Downloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<5,>=4.21.1 (from kfp)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.1)\n",
            "Collecting requests-toolbelt<1,>=0.8.0 (from kfp)\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Collecting urllib3<2.0.0 (from kfp)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.63.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (67.7.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n",
            "Building wheels for collected packages: kfp, kfp-server-api\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610419 sha256=372286987286528bc8476aeff4ab23cd5301179c66671bc910755779d22d0d5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-2.0.5-py3-none-any.whl size=114733 sha256=fe98a0f8c34795df50c883dea8e2e15425373e29ae1474cea6dad37dc064d118\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/4f/f0/2f622aadcbf8921fb72d24f52efaffacc235f863c195c289c5\n",
            "Successfully built kfp kfp-server-api\n",
            "Installing collected packages: urllib3, protobuf, kfp-server-api, kfp-pipeline-spec, requests-toolbelt, kubernetes, kfp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kfp-2.7.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 protobuf-4.25.3 requests-toolbelt-0.10.1 urllib3-1.26.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "cca0ef27acf04ff38ca78f12a9e6b088"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "9HVTHl85nCBP",
        "outputId": "9a25ea37-af5c-4437-cb24-9bdb9bd6e614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
        "! pip3 freeze | grep aiplatform"
      ],
      "metadata": {
        "id": "Ks1kCVEN7J86",
        "outputId": "a0d4ac8c-e5c7-4948-b37f-fa21243da253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFP SDK version: 2.7.0\n",
            "google-cloud-aiplatform==1.48.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"poised-conduit-420907\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "nXm3Z0zukbvv",
        "outputId": "aa9ee248-94cf-41bb-ab5d-d6f0566d867c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"us-central1\"\n",
        "BQ_REGION = REGION.split(\"-\")[0].upper()"
      ],
      "metadata": {
        "id": "EkG73jAeknuG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "5TQvaTwQm2sS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_URI = f\"gs://my-vertex-ai-project-{PROJECT_ID}-unique-20240421\""
      ],
      "metadata": {
        "id": "yctCn47InO5a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "SERVICE_ACCOUNT = \"709551633939-compute@developer.gserviceaccaunt.com\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "KzamG2AEo4w_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"709551633939-compute@developer.gserviceaccaunt.com\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ],
      "metadata": {
        "id": "Gpq5-mr4pCu7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator"
      ],
      "metadata": {
        "id": "eKnKKv92pXb3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "import kfp\n",
        "from kfp import compiler, dsl\n",
        "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
      ],
      "metadata": {
        "id": "6hDZUEE_pgBS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "H0vbGuMfpmw1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = %env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "\n",
        "DATASET_ID = \"coffe\"  # The Data Set ID where the view sits\n",
        "VIEW_NAME = \"coffe_data\"  # BigQuery view you create for input data\n",
        "\n",
        "KFP_ENDPOINT = (\n",
        "    \"709551633939\"\n",
        ")\n",
        "\n",
        "PIPELINE_ROOT = f\"{BUCKET_URI}/census_pipeline\"  # This is where all pipeline artifacts are sent. You'll need to ensure the bucket is created ahead of time\n",
        "PIPELINE_ROOT\n",
        "print(f\"PIPELINE_ROOT: {PIPELINE_ROOT}\")"
      ],
      "metadata": {
        "id": "PSR0yMDYpv3c",
        "outputId": "e3edecdd-3f1e-40b5-d1dc-f11c34d69917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PATH=/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n",
            "PIPELINE_ROOT: gs://my-vertex-ai-project-poised-conduit-420907-unique-20240421/census_pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bq mk --location=US  my_dataset"
      ],
      "metadata": {
        "id": "_2Dp07XPslmh",
        "outputId": "0c6e2653-99fd-4b0b-ab5c-527e5ce5c386",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'poised-conduit-420907:my_dataset' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\"google-cloud-bigquery==3.10.0\"],\n",
        ")\n",
        "def coffe_view(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    view_name: str,\n",
        "):\n",
        "    \"\"\"Creates a BigQuery view on `bigquery-public-data.ml_datasets.census_adult_income`.\n",
        "\n",
        "    Args:\n",
        "        project_id: The Project ID.\n",
        "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
        "        view_name: The BigQuery view name.\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    create_or_replace_view = \"\"\"\n",
        "        CREATE OR REPLACE VIEW\n",
        "        `{dataset_id}`.`{view_name}` AS\n",
        "        SELECT\n",
        "          age,\n",
        "          workclass,\n",
        "          education,\n",
        "          education_num,\n",
        "          marital_status,\n",
        "          occupation,\n",
        "          relationship,\n",
        "          race,\n",
        "          sex,\n",
        "          capital_gain,\n",
        "          capital_loss,\n",
        "          hours_per_week,\n",
        "          native_country,\n",
        "          income_bracket,\n",
        "        FROM\n",
        "          `bigquery-public-data.ml_datasets.census_adult_income`\n",
        "    \"\"\".format(\n",
        "        dataset_id=dataset_id, view_name=view_name\n",
        "    )\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    query_job = client.query(query=create_or_replace_view, job_config=job_config)\n",
        "    query_job.result()"
      ],
      "metadata": {
        "id": "cPYkTCiOszjf",
        "outputId": "10ee404a-ff65-4811-cf13-3a3f5387e47a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\"google-cloud-bigquery[pandas]==3.10.0\"],\n",
        ")\n",
        "def export_dataset(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    view_name: str,\n",
        "    dataset: Output[Dataset],\n",
        "):\n",
        "    \"\"\"Exports from BigQuery to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        project_id: The Project ID.\n",
        "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
        "        view_name: The BigQuery view name.\n",
        "\n",
        "    Returns:\n",
        "        dataset: The Dataset artifact with exported CSV file.\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    table_name = f\"{project_id}.{dataset_id}.{view_name}\"\n",
        "    query = \"\"\"\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{table_name}`\n",
        "    \"\"\".format(\n",
        "        table_name=table_name\n",
        "    )\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    query_job = client.query(query=query, job_config=job_config)\n",
        "    df = query_job.result().to_dataframe()\n",
        "    df.to_csv(dataset.path, index=False)\n",
        ""
      ],
      "metadata": {
        "id": "QtyqtcA1s0Hy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\n",
        "        \"xgboost==1.6.2\",\n",
        "        \"pandas==1.3.5\",\n",
        "        \"joblib==1.1.0\",\n",
        "        \"scikit-learn==1.0.2\",\n",
        "    ],\n",
        ")\n",
        "def xgboost_training(\n",
        "    dataset: Input[Dataset],\n",
        "    model: Output[Model],\n",
        "    metrics: Output[Metrics],\n",
        "):\n",
        "    \"\"\"Trains an XGBoost classifier.\n",
        "\n",
        "    Args:\n",
        "        dataset: The training dataset.\n",
        "\n",
        "    Returns:\n",
        "        model: The model artifact stores the model.joblib file.\n",
        "        metrics: The metrics of the trained model.\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import (accuracy_score, precision_recall_curve,\n",
        "                                 roc_auc_score)\n",
        "    from sklearn.model_selection import (RandomizedSearchCV, StratifiedKFold,\n",
        "                                         train_test_split)\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    # Load the training census dataset\n",
        "    with open(dataset.path, \"r\") as train_data:\n",
        "        raw_data = pd.read_csv(train_data)\n",
        "\n",
        "    CATEGORICAL_COLUMNS = (\n",
        "        \"workclass\",\n",
        "        \"education\",\n",
        "        \"marital_status\",\n",
        "        \"occupation\",\n",
        "        \"relationship\",\n",
        "        \"race\",\n",
        "        \"sex\",\n",
        "        \"native_country\",\n",
        "    )\n",
        "    LABEL_COLUMN = \"income_bracket\"\n",
        "    POSITIVE_VALUE = \" >50K\"\n",
        "\n",
        "    # Convert data in categorical columns to numerical values\n",
        "    encoders = {col: LabelEncoder() for col in CATEGORICAL_COLUMNS}\n",
        "    for col in CATEGORICAL_COLUMNS:\n",
        "        raw_data[col] = encoders[col].fit_transform(raw_data[col])\n",
        "\n",
        "    X = raw_data.drop([LABEL_COLUMN], axis=1).values\n",
        "    y = raw_data[LABEL_COLUMN] == POSITIVE_VALUE\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "    _ = xgb.DMatrix(X_train, label=y_train)\n",
        "    _ = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    params = {\n",
        "        \"reg_lambda\": [0, 1],\n",
        "        \"gamma\": [1, 1.5, 2, 2.5, 3],\n",
        "        \"max_depth\": [2, 3, 4, 5, 10, 20],\n",
        "        \"learning_rate\": [0.1, 0.01],\n",
        "    }\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=50,\n",
        "        objective=\"binary:hinge\",\n",
        "        silent=True,\n",
        "        nthread=1,\n",
        "        eval_metric=\"auc\",\n",
        "    )\n",
        "\n",
        "    folds = 5\n",
        "    param_comb = 20\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        xgb_model,\n",
        "        param_distributions=params,\n",
        "        n_iter=param_comb,\n",
        "        scoring=\"precision\",\n",
        "        n_jobs=4,\n",
        "        cv=skf.split(X_train, y_train),\n",
        "        verbose=4,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    random_search.fit(X_train, y_train)\n",
        "    xgb_model_best = random_search.best_estimator_\n",
        "    predictions = xgb_model_best.predict(X_test)\n",
        "    score = accuracy_score(y_test, predictions)\n",
        "    auc = roc_auc_score(y_test, predictions)\n",
        "    _ = precision_recall_curve(y_test, predictions)\n",
        "\n",
        "    metrics.log_metric(\"accuracy\", (score * 100.0))\n",
        "    metrics.log_metric(\"framework\", \"xgboost\")\n",
        "    metrics.log_metric(\"dataset_size\", len(raw_data))\n",
        "    metrics.log_metric(\"AUC\", auc)\n",
        "\n",
        "    # Export the model to a file\n",
        "    os.makedirs(model.path, exist_ok=True)\n",
        "    joblib.dump(xgb_model_best, os.path.join(model.path, \"model.joblib\"))"
      ],
      "metadata": {
        "id": "4_ZM6ckus61U"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
        ")\n",
        "def deploy_xgboost_model(\n",
        "    model: Input[Model],\n",
        "    project_id: str,\n",
        "    vertex_endpoint: Output[Artifact],\n",
        "    vertex_model: Output[Model],\n",
        "):\n",
        "    \"\"\"Deploys an XGBoost model to Vertex AI Endpoint.\n",
        "\n",
        "    Args:\n",
        "        model: The model to deploy.\n",
        "        project_id: The project ID of the Vertex AI Endpoint.\n",
        "\n",
        "    Returns:\n",
        "        vertex_endpoint: The deployed Vertex AI Endpoint.\n",
        "        vertex_model: The deployed Vertex AI Model.\n",
        "    \"\"\"\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    aiplatform.init(project=project_id)\n",
        "\n",
        "    deployed_model = aiplatform.Model.upload(\n",
        "        display_name=\"census-demo-model\",\n",
        "        artifact_uri=model.uri,\n",
        "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n",
        "    )\n",
        "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
        "\n",
        "    vertex_endpoint.uri = endpoint.resource_name\n",
        "    vertex_model.uri = deployed_model.resource_name"
      ],
      "metadata": {
        "id": "b5NoqYNytEW8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"census-demo-pipeline\",\n",
        ")\n",
        "def pipeline():\n",
        "    \"\"\"A demo pipeline.\"\"\"\n",
        "\n",
        "    create_input_view_task = coffe_view(\n",
        "        project_id=PROJECT_ID,\n",
        "        dataset_id=DATASET_ID,\n",
        "        view_name=VIEW_NAME,\n",
        "    )\n",
        "\n",
        "    export_dataset_task = (\n",
        "        export_dataset(\n",
        "            project_id=PROJECT_ID,\n",
        "            dataset_id=DATASET_ID,\n",
        "            view_name=VIEW_NAME,\n",
        "        )\n",
        "        .after(create_input_view_task)\n",
        "        .set_caching_options(False)\n",
        "    )\n",
        "\n",
        "    training_task = xgboost_training(\n",
        "        dataset=export_dataset_task.outputs[\"dataset\"],\n",
        "    )\n",
        "\n",
        "    _ = deploy_xgboost_model(\n",
        "        project_id=PROJECT_ID,\n",
        "        model=training_task.outputs[\"model\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "CUq4JtXBtIr_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.yaml\")"
      ],
      "metadata": {
        "id": "ddFiaQ75tN7A"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"census-demo-pipeline\",\n",
        "    template_path=\"pipeline.yaml\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "job.run()\n",
        "\n"
      ],
      "metadata": {
        "id": "Gcd86hIStR9J",
        "outputId": "c287fbff-2cd8-43ad-afec-81e7a2186084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/census-demo-pipeline-20240421141017?project=709551633939\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/709551633939/locations/us-central1/pipelineJobs/census-demo-pipeline-20240421141017 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [coffe-view].; Job (project_id = poised-conduit-420907, job_id = 4454386861300776960) is failed due to the above error.; Failed to handle the job: {project_number = 709551633939, job_id = 4454386861300776960}\"\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e7406bef7bee>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         self._run(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     def submit(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [coffe-view].; Job (project_id = poised-conduit-420907, job_id = 4454386861300776960) is failed due to the above error.; Failed to handle the job: {project_number = 709551633939, job_id = 4454386861300776960}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    KFP_ENDPOINT = \"709551633939\"\n",
        "\n",
        "    client = kfp.Client(\n",
        "        host=KFP_ENDPOINT,\n",
        "    )\n",
        "\n",
        "    client.create_run_from_pipeline_package(\n",
        "        pipeline_file=\"pipeline.yaml\",\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        run_name=\"census-demo-pipeline\",\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "7hZCNBhTtWLz",
        "outputId": "1b1c19c3-d705-410b-e942-7045a871de10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/kfp/client/client.py:243: UserWarning: The host 709551633939 does not contain the \"http\" or \"https\" protocol. Defaults to \"https\".\n",
            "  warnings.warn(\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd55eb8b640>: Failed to establish a new connection: [Errno -2] Name or service not known')': /apis/v2beta1/healthz\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd55eb8a830>: Failed to establish a new connection: [Errno -2] Name or service not known')': /apis/v2beta1/healthz\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd55eb8b970>: Failed to establish a new connection: [Errno -2] Name or service not known')': /apis/v2beta1/healthz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTPSConnectionPool(host='709551633939', port=443): Max retries exceeded with url: /apis/v2beta1/healthz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd55eb8beb0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1AnNNVzRJBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}